{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf0bea2",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "1. We load finite element results files.\n",
    "1. We split data in train/validation/test.\n",
    "1. We compute some statistic.\n",
    "1. We save all in one master file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322cd410",
   "metadata": {},
   "source": [
    "### General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53da503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================= CONFIG =========================\n",
    "DATA_DIR = Path(\"./data/Run1\")\n",
    "MASTER_FOLDER = Path(\"master_data\")\n",
    "MASTER_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "MASTER_FILE = MASTER_FOLDER / \"rve_run1.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ecc0b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RVE files found: 1000\n",
      "Image dimensions (H x W): 96 x 96\n",
      "Time steps per RVE: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get list of .h5 files in the data directory\n",
    "h5_files = sorted(list(DATA_DIR.glob(\"*.h5\")))\n",
    "\n",
    "N_RVE = len(h5_files)            # Total number of RVE files available\n",
    "\n",
    "# Take dimensions and number of steps from the first file\n",
    "first_file = h5_files[0]\n",
    "with h5py.File(first_file, 'r') as f:\n",
    "    # Read metadata attributes for dimensions and steps\n",
    "    H = int(f.attrs['H_gp'])\n",
    "    W = int(f.attrs['W_gp'])\n",
    "    N_STEPS_PER_RVE = int(f.attrs['n_steps'])  # Time steps to use from each RVE\n",
    "\n",
    "print(f'Total RVE files found: {N_RVE}')\n",
    "print(f'Image dimensions (H x W): {H} x {W}')\n",
    "print(f'Time steps per RVE: {N_STEPS_PER_RVE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce5f5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RVEs: 600 (f=0.60) | Val: 200  (f=0.20) | Test: 200  (f=0.20)\n",
      "Total samples: 100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ====================== TRAIN / VALIDATION / TEST ======================\n",
    "\n",
    "rnd_seed = 42\n",
    "\n",
    "# Define fractions for train/validation/test\n",
    "val_size = 0.20\n",
    "test_size = 0.20\n",
    "train_size = 1 - (val_size + test_size)\n",
    "\n",
    "# Generate sets\n",
    "rve_indexs = list(range(1, N_RVE + 1))\n",
    "train_val_rves, test_rves = train_test_split(rve_indexs,\n",
    "                                                   test_size = test_size,\n",
    "                                                   random_state = rnd_seed)\n",
    "\n",
    "train_rves, val_rves = train_test_split(train_val_rves,\n",
    "                                            test_size=val_size / (train_size + val_size),\n",
    "                                            random_state = rnd_seed)\n",
    "\n",
    "# Compute amount of elements per set\n",
    "ntrain = len(train_rves)\n",
    "nval = len(val_rves)\n",
    "ntest = len(test_rves)\n",
    "\n",
    "print(f\"Train RVEs: {ntrain} (f={train_size:.2f}) | Val: {nval}  (f={val_size:.2f}) | Test: {ntest}  (f={test_size:.2f})\")\n",
    "print(f\"Total samples: {N_RVE * N_STEPS_PER_RVE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9386fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train split (60000 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [38:40<00:00,  3.87s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing val split (20000 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [12:45<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test split (20000 samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [13:46<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Master HDF5 created successfully:\n",
      "   → master_data\\rve_run1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====================== CREATE MASTER HDF5 ======================\n",
    "\n",
    "\"\"\"\n",
    "Create Master HDF5 dataset for Dual-Encoder FNO\n",
    "- x_local  : phase field (0=soft, 1=hard)\n",
    "- x_global : [exx, eyy, gxy, E_soft, nu_soft, E_hard, nu_hard]\n",
    "- y_local  : [Sxx, Syy, Sxy]\n",
    "\"\"\"\n",
    "\n",
    "# Auxiliary function to load RVE data from a single file\n",
    "def load_rve_data(file_path):\n",
    "    with h5py.File(file_path, 'r') as src:\n",
    "        fields = src['fields'][:]\n",
    "        macro  = src['macro'][:]\n",
    "    return fields, macro\n",
    "\n",
    "workers = 8\n",
    "\n",
    "with h5py.File(MASTER_FILE, 'w') as f:\n",
    "    for split_name, rve_list in [(\"train\", train_rves), \n",
    "                                 (\"val\",   val_rves), \n",
    "                                 (\"test\",  test_rves)]:\n",
    "        \n",
    "        N_split = len(rve_list) * N_STEPS_PER_RVE\n",
    "        g = f.create_group(split_name)\n",
    "        \n",
    "        g.create_dataset('x_local',  shape=(N_split, H, W, 1), dtype=np.float32, compression='gzip', chunks=True)\n",
    "        g.create_dataset('x_global', shape=(N_split, 7), dtype=np.float32, compression='gzip', chunks=True)\n",
    "        g.create_dataset('y_local',  shape=(N_split, H, W, 3), dtype=np.float32, compression='gzip', chunks=True)\n",
    "        \n",
    "        idx = 0\n",
    "        print(f\"\\nProcessing {split_name} split ({N_split} samples)...\")\n",
    "        \n",
    "        # Pre-compute paths\n",
    "        file_paths = [DATA_DIR / f\"rve_{rve_id:04d}.h5\" for rve_id in rve_list]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            \n",
    "            results = executor.map(load_rve_data, file_paths)\n",
    "            \n",
    "            for fields, macro in tqdm(results, total=len(file_paths)):\n",
    "                n_steps = min(N_STEPS_PER_RVE, len(fields))\n",
    "                \n",
    "                start_idx = idx\n",
    "                end_idx = idx + n_steps\n",
    "                \n",
    "                g['x_local'][start_idx:end_idx] = fields[:n_steps, :, :, 0:1]\n",
    "                g['x_global'][start_idx:end_idx] = macro[:n_steps]\n",
    "                g['y_local'][start_idx:end_idx] = fields[:n_steps, :, :, 1:4]\n",
    "                \n",
    "                idx = end_idx\n",
    "\n",
    "print(f\"\\n✅ Master HDF5 created successfully:\")\n",
    "print(f\"   → {MASTER_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc78ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing normalization statistics (from train only)...\n",
      "   Statistics saved in group '/stats'\n",
      "\n",
      "✅ Master dataset is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# ====================== COMPUTE AND SAVE STATS ======================\n",
    "\n",
    "print(\"\\nComputing normalization statistics (from train only)...\")\n",
    "\n",
    "with h5py.File(MASTER_FILE, 'a') as f:\n",
    "    train = f['train']\n",
    "    \n",
    "    x_local  = train['x_local'][:]\n",
    "    x_global = train['x_global'][:]\n",
    "    y_local  = train['y_local'][:]\n",
    "    \n",
    "    stats = f.create_group('stats')\n",
    "    \n",
    "    stats.create_dataset('mean_x_local',  data=x_local.mean(axis=(0,1,2), keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_x_local',   data=x_local.std(axis=(0,1,2), keepdims=True).squeeze(0) + 1e-8)\n",
    "    \n",
    "    stats.create_dataset('mean_x_global', data=x_global.mean(axis=0, keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_x_global',  data=x_global.std(axis=0, keepdims=True).squeeze(0) + 1e-8)\n",
    "    \n",
    "    stats.create_dataset('mean_y_local',  data=y_local.mean(axis=(0,1,2), keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_y_local',   data=y_local.std(axis=(0,1,2), keepdims=True).squeeze(0) + 1e-8)\n",
    "\n",
    "print(\"   Statistics saved in group '/stats'\")\n",
    "print(\"\\n✅ Master dataset is ready for training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (feinn-project)",
   "language": "python",
   "name": "feinn-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
