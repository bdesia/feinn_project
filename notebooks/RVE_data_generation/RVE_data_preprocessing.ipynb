{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf0bea2",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "1. We load finite element results files.\n",
    "1. We split data in train/validation/test.\n",
    "1. We compute some statistic.\n",
    "1. We save all in one master file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322cd410",
   "metadata": {},
   "source": [
    "### General Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f285b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53da503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========================= CONFIG =========================\n",
    "DATA_DIR = Path(\"./data/Run1\")\n",
    "MASTER_FILE = Path(\"./master_data/rve_run1.h5\")\n",
    "\n",
    "N_RVE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get list of .h5 files in the data directory\n",
    "h5_files = sorted(list(DATA_DIR.glob(\"*.h5\")))\n",
    "\n",
    "N_RVE = len(h5_files)            # Total number of RVE files available\n",
    "\n",
    "# Take dimensions and number of steps from the first file\n",
    "first_file = h5_files[0]\n",
    "with h5py.File(first_file, 'r') as f:\n",
    "    # Read metadata attributes for dimensions and steps\n",
    "    H = int(f.attrs['H_gp'])\n",
    "    W = int(f.attrs['W_gp'])\n",
    "    N_STEPS_PER_RVE = int(f.attrs['n_steps'])  # Time steps to use from each RVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ====================== TRAIN / VALIDATION / TEST ======================\n",
    "\n",
    "rnd_seed = 42\n",
    "\n",
    "# Define fractions for train/validation/test\n",
    "val_size = 0.20\n",
    "test_size = 0.20\n",
    "train_size = 1 - (val_size + test_size)\n",
    "\n",
    "# Generate sets\n",
    "rve_indexs = list(range(1, N_RVE + 1))\n",
    "train_val_rves, test_rves = train_test_split(rve_indexs,\n",
    "                                                   test_size = test_size,\n",
    "                                                   random_state = rnd_seed)\n",
    "\n",
    "train_rves, val_rves = train_test_split(train_val_rves,\n",
    "                                            test_size=val_size / (train_size + val_size),\n",
    "                                            random_state = rnd_seed)\n",
    "\n",
    "# Compute amount of elements per set\n",
    "ntrain = len(train_rves)\n",
    "nval = len(val_rves)\n",
    "ntest = len(test_rves)\n",
    "\n",
    "print(f\"Train RVEs: {ntrain} (f={train_size:.2f}) | Val: {nval}  (f={val_size:.2f}) | Test: {ntest}  (f={test_size:.2f})\")\n",
    "print(f\"Total samples: {N_RVE * N_STEPS_PER_RVE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ====================== CREATE MASTER HDF5 ======================\n",
    "\n",
    "\"\"\"\n",
    "Create Master HDF5 dataset for Dual-Encoder FNO\n",
    "- x_local  : phase field (0=soft, 1=hard)\n",
    "- x_global : [exx, eyy, gxy, E_soft, nu_soft, E_hard, nu_hard]\n",
    "- y_local  : [Sxx, Syy, Sxy]\n",
    "\"\"\"\n",
    "\n",
    "with h5py.File(MASTER_FILE, 'w') as f:\n",
    "    for split_name, rve_list in [(\"train\", train_rves), \n",
    "                                 (\"val\",   val_rves), \n",
    "                                 (\"test\",  test_rves)]:\n",
    "        \n",
    "        N_split = len(rve_list) * N_STEPS_PER_RVE\n",
    "        g = f.create_group(split_name)\n",
    "        \n",
    "        g.create_dataset('x_local',  shape=(N_split, H, W, 1), dtype=np.float32, compression='gzip')\n",
    "        g.create_dataset('x_global', shape=(N_split, 7), dtype=np.float32, compression='gzip')\n",
    "        g.create_dataset('y_local',  shape=(N_split, H, W, 3), dtype=np.float32, compression='gzip')\n",
    "        \n",
    "        idx = 0\n",
    "        print(f\"\\nProcessing {split_name} split ({N_split} samples)...\")\n",
    "        \n",
    "        for rve_id in tqdm(rve_list):\n",
    "            file_path = DATA_DIR / f\"rve_{rve_id:04d}.h5\"\n",
    "            \n",
    "            with h5py.File(file_path, 'r') as src:\n",
    "                fields = src['fields'][:]      # (N_steps, H, W, 4)\n",
    "                macro  = src['macro'][:]       # (N_steps, 7)\n",
    "                \n",
    "                for step in range(min(N_STEPS_PER_RVE, len(fields))):\n",
    "                    # x_local = phase field\n",
    "                    g['x_local'][idx, :, :, 0] = fields[step, :, :, 0]\n",
    "                    \n",
    "                    # x_global\n",
    "                    g['x_global'][idx] = macro[step]\n",
    "                    \n",
    "                    # y_local = stress field\n",
    "                    g['y_local'][idx] = fields[step, :, :, 1:4]\n",
    "                    \n",
    "                    idx += 1\n",
    "\n",
    "print(f\"\\n✅ Master HDF5 created successfully:\")\n",
    "print(f\"   → {MASTER_FILE}\")\n",
    "print(f\"   Train: {len(train_rves)*N_STEPS_PER_RVE} samples\")\n",
    "print(f\"   Val:   {len(val_rves)*N_STEPS_PER_RVE} samples\")\n",
    "print(f\"   Test:  {len(test_rves)*N_STEPS_PER_RVE} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc78ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing normalization statistics (from train only)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create group (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m x_global = train[\u001b[33m'\u001b[39m\u001b[33mx_global\u001b[39m\u001b[33m'\u001b[39m][:]\n\u001b[32m      9\u001b[39m y_local  = train[\u001b[33m'\u001b[39m\u001b[33my_local\u001b[39m\u001b[33m'\u001b[39m][:]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m stats = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_group\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m stats.create_dataset(\u001b[33m'\u001b[39m\u001b[33mmean_x_local\u001b[39m\u001b[33m'\u001b[39m,  data=x_local.mean(axis=(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m), keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     14\u001b[39m stats.create_dataset(\u001b[33m'\u001b[39m\u001b[33mstd_x_local\u001b[39m\u001b[33m'\u001b[39m,   data=x_local.std(axis=(\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m), keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[32m1e-8\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Braian\\OneDrive\\Escritorio\\GitHub\\CEIA\\feinn_project\\.venv\\Lib\\site-packages\\h5py\\_hl\\group.py:71\u001b[39m, in \u001b[36mGroup.create_group\u001b[39m\u001b[34m(self, name, track_order, track_times)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtrack_times must be either True, False, or None\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m gid = \u001b[43mh5g\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlcpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Group(gid)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5g.pyx:173\u001b[39m, in \u001b[36mh5py.h5g.create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to synchronously create group (name already exists)"
     ]
    }
   ],
   "source": [
    "# ====================== COMPUTE AND SAVE STATS ======================\n",
    "\n",
    "print(\"\\nComputing normalization statistics (from train only)...\")\n",
    "\n",
    "with h5py.File(MASTER_FILE, 'a') as f:\n",
    "    train = f['train']\n",
    "    \n",
    "    x_local  = train['x_local'][:]\n",
    "    x_global = train['x_global'][:]\n",
    "    y_local  = train['y_local'][:]\n",
    "    \n",
    "    stats = f.create_group('stats')\n",
    "    \n",
    "    stats.create_dataset('mean_x_local',  data=x_local.mean(axis=(0,1,2), keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_x_local',   data=x_local.std(axis=(0,1,2), keepdims=True).squeeze(0) + 1e-8)\n",
    "    \n",
    "    stats.create_dataset('mean_x_global', data=x_global.mean(axis=0, keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_x_global',  data=x_global.std(axis=0, keepdims=True).squeeze(0) + 1e-8)\n",
    "    \n",
    "    stats.create_dataset('mean_y_local',  data=y_local.mean(axis=(0,1,2), keepdims=True).squeeze(0))\n",
    "    stats.create_dataset('std_y_local',   data=y_local.std(axis=(0,1,2), keepdims=True).squeeze(0) + 1e-8)\n",
    "\n",
    "print(\"   Statistics saved in group '/stats'\")\n",
    "print(\"\\n✅ Master dataset is ready for training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feinn-project-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
